\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}

\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}

% Configuración para bloques de código
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle,
    literate={á}{{\'a}}1
             {é}{{\'e}}1
             {í}{{\'i}}1
             {ó}{{\'o}}1
             {ú}{{\'u}}1
             {ñ}{{\~n}}1
             {Á}{{\'A}}1
             {É}{{\'E}}1
             {Í}{{\'I}}1
             {Ó}{{\'O}}1
             {Ú}{{\'U}}1
             {Ñ}{{\~N}}1
}

\lstdefinelanguage{yaml}{
  keywords={true,false,null,y,n},
  keywordstyle=\color{magenta},
  sensitive=false,
  comment=[l]{\#},
  commentstyle=\color{codegreen},
  stringstyle=\color{codepurple},
  morestring=[b]',
  morestring=[b]'
}

\title{\textbf{Implementación de Memoria Persistente en Agentes de IA}\\
\large Una Arquitectura Práctica basada en Bases de Datos Vectoriales}
\author{Equipo de Desarrollo de Agentes Autónomos}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introducción}
Los Modelos de Lenguaje Grande (LLMs) poseen una limitación fundamental: su 'memoria' es efímera y está restringida por la ventana de contexto. Una vez que una sesión termina o el contexto se llena, el conocimiento adquirido durante la interacción se pierde.

Este documento detalla una solución arquitectónica para dotar a los agentes de IA de una \textbf{memoria a largo plazo} (Long-Term Memory). Utilizando una base de datos vectorial local (\textit{ChromaDB}) y scripts deterministas en Python, logramos que el agente almacene aprendizajes, soluciones a errores y preferencias del usuario, recuperándolos semánticamente en futuras sesiones.

\section{Arquitectura del Sistema}
El sistema sigue una arquitectura de tres capas diseñada para separar la intención probabilística (LLM) de la ejecución determinista (Código).

\begin{itemize}
    \item \textbf{Capa 1: Directivas (YAML):} Definen el 'qué hacer'. Son los procedimientos operativos estándar.
    \item \textbf{Capa 2: Orquestación (LLM):} El cerebro que decide cuándo guardar o consultar la memoria.
    \item \textbf{Capa 3: Ejecución (Python):} Scripts que interactúan físicamente con la base de datos.
\end{itemize}

\section{Requisitos Previos}
Para implementar este sistema, se requiere un entorno aislado para manejar las dependencias de Python.

\subsection{Configuración del Entorno}
\begin{lstlisting}[language=bash, caption=Creación del entorno con Conda]
# 1. Crear el entorno
conda create --name agent_env python=3.10 -y

# 2. Activar el entorno
conda activate agent_env

# 3. Instalar dependencias
pip install chromadb
\end{lstlisting}

\section{Implementación Técnica (Capa de Ejecución)}

\subsection{Guardado de Memoria (\texttt{save\_memory.py})}
Este script se encarga de vectorizar el texto y almacenarlo en \textit{ChromaDB}. Añade metadatos cruciales como la categoría y la marca de tiempo para facilitar la gestión futura.

\begin{lstlisting}[language=Python, caption=Fragmento clave de save\_memory.py]
import chromadb
import uuid
import datetime

# Inicialización persistente
client = chromadb.PersistentClient(path='.tmp/chroma_db')
collection = client.get_or_create_collection(name='agent_memory')

# Generación de metadatos
memory_id = str(uuid.uuid4())
timestamp = datetime.datetime.now().isoformat()
metadata = {
    'category': args.category, # Ej: 'error_fix', 'user_preference'
    'timestamp': timestamp,
    'source': 'user_input'
}

# Upsert (Insertar o Actualizar)
collection.add(
    documents=[args.text],
    metadatas=[metadata],
    ids=[memory_id]
)
\end{lstlisting}

\subsection{Consulta Semántica (\texttt{query\_memory.py})}
A diferencia de una búsqueda por palabras clave (Ctrl+F), este script busca por \textit{similitud semántica}. Si buscamos 'problema de módulos', el sistema encontrará registros sobre 'ModuleNotFound' aunque las palabras no sean idénticas.

\begin{lstlisting}[language=Python, caption=Lógica de búsqueda semántica]
results = collection.query(
    query_texts=[args.query],
    n_results=3 # Top 3 resultados más relevantes
)

# El resultado incluye la 'distancia', donde menor valor 
# implica mayor similitud semántica.
\end{lstlisting}

\subsection{Gestión de Recuerdos (\texttt{list\_memories.py})}
Para auditoría y mantenimiento, es necesario poder listar los recuerdos cronológicamente sin realizar búsquedas vectoriales. Dado que ChromaDB no ordena nativamente por metadatos en la consulta básica, lo hacemos en Python.

\begin{lstlisting}[language=Python, caption=Listado cronológico]
data = collection.get() # Obtener todo

# Estructurar datos
memories = []
for i in range(len(data['ids'])):
    memories.append({
        'id': data['ids'][i],
        'content': data['documents'][i],
        'timestamp': data['metadatas'][i].get('timestamp', '')
    })

# Ordenar por fecha descendente (más reciente primero)
memories.sort(key=lambda x: x['timestamp'], reverse=True)
\end{lstlisting}

\section{Integración (Capa de Directivas)}
Las directivas YAML exponen estas capacidades al orquestador.

\subsection{Directiva de Guardado (\texttt{save\_memory.yaml})}
\begin{lstlisting}[language=yaml]
goal: 'Guardar un fragmento de información en la memoria a largo plazo.'
required_inputs:
  - name: 'content'
    description: 'La información textual a recordar.'
optional_inputs:
  - name: 'category'
    default: 'general'
steps:
  - step: 1
    script_to_invoke: 'execution/save_memory.py'
    inputs:
      - name: '--text'
        value: '{{content}}'
      - name: '--category'
        value: '{{category}}'
\end{lstlisting}

\subsection{Directiva de Consulta (\texttt{query\_memory.yaml})}
\begin{lstlisting}[language=yaml]
goal: 'Recuperar información relevante basada en una consulta semántica.'
required_inputs:
  - name: 'query'
    description: 'El contexto sobre el cual se busca información.'
steps:
  - step: 1
    script_to_invoke: 'execution/query_memory.py'
    inputs:
      - name: '--query'
        value: '{{query}}'
\end{lstlisting}

\section{Caso de Uso Práctico: Resolución de Errores Recurrentes}

\subsection{Escenario}
Un desarrollador encuentra un error complejo durante la compilación y encuentra la solución tras horas de investigación.

\subsection{Flujo de Trabajo}
\begin{enumerate}
    \item \textbf{Incidente:} El usuario reporta: 'El error 'ModuleNotFound' se soluciona activando el entorno conda'.
    \item \textbf{Acción del Agente:} El orquestador detecta un aprendizaje útil e invoca \texttt{save\_memory.yaml}.
    \begin{itemize}
        \item \texttt{content}: 'El error 'ModuleNotFound' se soluciona activando el entorno conda'
        \item \texttt{category}: 'error\_fix'
    \end{itemize}
    \item \textbf{Persistencia:} El script guarda el vector en \texttt{.tmp/chroma\_db}.
    \item \textbf{Recuperación (Meses después):} El usuario pregunta: '¿Por qué me dice que faltan librerías?'.
    \item \textbf{Consulta:} El orquestador invoca \texttt{query\_memory.yaml} con la query 'faltan librerías'.
    \item \textbf{Resultado:} ChromaDB devuelve el recuerdo sobre 'ModuleNotFound' debido a la alta similitud semántica, permitiendo al agente dar la solución correcta inmediatamente.
\end{enumerate}

\section{Conclusión}
La implementación de una memoria persistente mediante bases de datos vectoriales transforma a los agentes de IA de simples procesadores de texto a sistemas capaces de aprendizaje incremental. Esta arquitectura modular asegura que el conocimiento del equipo se preserve y sea accesible, aumentando la eficiencia operativa y reduciendo la redundancia en la resolución de problemas.

\end{document}